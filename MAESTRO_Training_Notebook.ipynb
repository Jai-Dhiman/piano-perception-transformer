{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "title"
   },
   "source": [
    "# Piano Perception Transformer - MAESTRO Training Pipeline\n",
    "\n",
    "This notebook implements the complete training pipeline:\n",
    "1. **SSAST Pre-training** on MAESTRO dataset\n",
    "2. **AST Fine-tuning** on PercePiano dataset\n",
    "\n",
    "**Runtime Requirements:**\n",
    "- Use **TPU v2-8**\n",
    "\n",
    "**Memory-Efficient Approach:**\n",
    "- Streaming MAESTRO processing (avoids 200GB storage limit)\n",
    "- Only keeps processed spectrograms (~10GB vs 200GB raw audio)\n",
    "- Automatic cleanup of raw audio files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "setup"
   },
   "outputs": [],
   "source": [
    "# Cell 1: Initial Setup\n",
    "print(\"üöÄ Setting up Piano Perception Transformer...\")\n",
    "\n",
    "# Clone repo (skip if already exists)\n",
    "import os\n",
    "if not os.path.exists('piano-perception-transformer'):\n",
    "    !git clone https://github.com/Jai-Dhiman/piano-perception-transformer.git\n",
    "else:\n",
    "    print(\"Repository already exists, skipping clone...\")\n",
    "\n",
    "%cd piano-perception-transformer\n",
    "\n",
    "# Install uv\n",
    "!curl -LsSf https://astral.sh/uv/install.sh | sh\n",
    "\n",
    "# Fix the path and install dependencies\n",
    "print(\"üì¶ Installing dependencies with uv...\")\n",
    "!export PATH=\"/usr/local/bin:$PATH\" && uv pip install --system jax[tpu] flax optax librosa pandas wandb requests zipfile36\n",
    "\n",
    "print(\"‚úÖ Setup completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kaggle_setup"
   },
   "outputs": [],
   "source": [
    "# Cell 2: Mount Google Drive & Setup Memory-Efficient Storage\n",
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "print(\"üíæ Mounting Google Drive for persistent storage...\")\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create directories for saving processed data and checkpoints\n",
    "!mkdir -p /content/drive/MyDrive/piano_transformer\n",
    "!mkdir -p /content/drive/MyDrive/piano_transformer/processed_spectrograms\n",
    "!mkdir -p /content/drive/MyDrive/piano_transformer/checkpoints\n",
    "!mkdir -p /content/drive/MyDrive/piano_transformer/logs\n",
    "!mkdir -p /content/drive/MyDrive/piano_transformer/temp\n",
    "\n",
    "print(\"‚úÖ Google Drive mounted and directories created!\")\n",
    "print(\"üìÅ Storage structure:\")\n",
    "!ls -la /content/drive/MyDrive/piano_transformer/\n",
    "\n",
    "# Check available space\n",
    "!df -h /content\n",
    "print(\"‚úÖ Storage setup completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "download_maestro"
   },
   "outputs": [],
   "source": [
    "# Cell 3: Streaming MAESTRO Processing\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "import librosa\n",
    "import numpy as np\n",
    "import zipfile\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "import sys\n",
    "from io import BytesIO\n",
    "sys.path.append('./src')\n",
    "\n",
    "print(\"üåä Starting streaming MAESTRO processing...\")\n",
    "\n",
    "def ensure_directories():\n",
    "    \"\"\"Create all necessary directories in Google Drive\"\"\"\n",
    "    directories = [\n",
    "        '/content/drive/MyDrive/piano_transformer',\n",
    "        '/content/drive/MyDrive/piano_transformer/processed_spectrograms',\n",
    "        '/content/drive/MyDrive/piano_transformer/checkpoints',\n",
    "        '/content/drive/MyDrive/piano_transformer/logs',\n",
    "        '/content/drive/MyDrive/piano_transformer/temp'\n",
    "    ]\n",
    "    \n",
    "    print(\"üìÅ Ensuring directory structure...\")\n",
    "    for directory in directories:\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "        print(f\"‚úÖ Created/verified: {directory}\")\n",
    "\n",
    "def download_and_process_maestro_streaming(max_files=None):\n",
    "    \"\"\"Download MAESTRO ZIP as stream, extract and process audio‚Üíspectrograms, save to Drive\"\"\"\n",
    "    \n",
    "    # Ensure directories exist first\n",
    "    ensure_directories()\n",
    "    \n",
    "    # Download metadata first to get real file paths\n",
    "    print(\"üìã Downloading MAESTRO metadata...\")\n",
    "    metadata_url = \"https://storage.googleapis.com/magentadata/datasets/maestro/v3.0.0/maestro-v3.0.0.json\"\n",
    "    \n",
    "    try:\n",
    "        metadata_response = requests.get(metadata_url, timeout=30)\n",
    "        metadata_response.raise_for_status()\n",
    "        maestro_metadata = metadata_response.json()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"‚ùå Failed to download metadata: {e}\")\n",
    "        raise Exception(f\"Cannot download MAESTRO metadata: {e}\")\n",
    "    \n",
    "    print(f\"üìä Found metadata for MAESTRO dataset\")\n",
    "    \n",
    "    # Save metadata to Drive\n",
    "    try:\n",
    "        with open('/content/drive/MyDrive/piano_transformer/maestro_metadata.json', 'w') as f:\n",
    "            json.dump(maestro_metadata, f)\n",
    "        print(\"‚úÖ Metadata saved to Drive\")\n",
    "    except IOError as e:\n",
    "        print(f\"‚ùå Failed to save metadata: {e}\")\n",
    "        raise Exception(f\"Cannot save metadata to Drive: {e}\")\n",
    "    \n",
    "    # MAESTRO v3.0.0 uses pandas-style JSON structure\n",
    "    if not isinstance(maestro_metadata, dict):\n",
    "        raise Exception(f\"Expected dict metadata, got {type(maestro_metadata)}\")\n",
    "    \n",
    "    # Check for required fields\n",
    "    required_fields = ['audio_filename', 'canonical_composer', 'canonical_title']\n",
    "    for field in required_fields:\n",
    "        if field not in maestro_metadata:\n",
    "            raise Exception(f\"Required field '{field}' not found in metadata. Available fields: {list(maestro_metadata.keys())}\")\n",
    "    \n",
    "    # Get the audio filenames from the pandas-style structure\n",
    "    audio_filenames = maestro_metadata['audio_filename']\n",
    "    if not isinstance(audio_filenames, dict):\n",
    "        raise Exception(f\"Expected dict for audio_filename field, got {type(audio_filenames)}\")\n",
    "    \n",
    "    total_files = len(audio_filenames)\n",
    "    print(f\"üìù Found {total_files} audio files in metadata\")\n",
    "    \n",
    "    # Get list of audio files to process\n",
    "    target_files = set()\n",
    "    files_to_process = list(audio_filenames.items())\n",
    "    if max_files:\n",
    "        files_to_process = files_to_process[:max_files]\n",
    "        print(f\"üéØ Processing first {max_files} files for demo/testing\")\n",
    "    else:\n",
    "        print(f\"üéØ Processing all {total_files} files\")\n",
    "    \n",
    "    for idx, filename in files_to_process:\n",
    "        if filename and isinstance(filename, str) and filename.endswith('.wav'):\n",
    "            target_files.add(filename)\n",
    "    \n",
    "    if not target_files:\n",
    "        raise Exception(\"No valid .wav files found in metadata\")\n",
    "    \n",
    "    print(f\"üéµ Target: {len(target_files)} audio files from ZIP\")\n",
    "    \n",
    "    # Download and stream process the MAESTRO ZIP\n",
    "    zip_url = \"https://storage.googleapis.com/magentadata/datasets/maestro/v3.0.0/maestro-v3.0.0.zip\"\n",
    "    print(f\"üì¶ Downloading MAESTRO ZIP stream from: {zip_url}\")\n",
    "    \n",
    "    processed_count = 0\n",
    "    \n",
    "    try:\n",
    "        # Stream download the ZIP file\n",
    "        with requests.get(zip_url, stream=True, timeout=300) as zip_response:\n",
    "            zip_response.raise_for_status()\n",
    "            \n",
    "            print(\"‚úÖ ZIP stream connected, processing...\")\n",
    "            \n",
    "            # Create a temporary file to hold the ZIP stream\n",
    "            with tempfile.NamedTemporaryFile(suffix='.zip') as temp_zip:\n",
    "                # Download ZIP in chunks to avoid memory issues\n",
    "                total_size = int(zip_response.headers.get('content-length', 0))\n",
    "                downloaded = 0\n",
    "                \n",
    "                print(f\"üìä ZIP size: {total_size / (1024**3):.1f}GB\")\n",
    "                \n",
    "                for chunk in zip_response.iter_content(chunk_size=8192 * 1024):  # 8MB chunks\n",
    "                    if chunk:\n",
    "                        temp_zip.write(chunk)\n",
    "                        downloaded += len(chunk)\n",
    "                        \n",
    "                        # Show progress every 1GB\n",
    "                        if downloaded % (1024**3) < (8192 * 1024):\n",
    "                            progress = (downloaded / total_size) * 100 if total_size > 0 else 0\n",
    "                            print(f\"üì• Downloaded: {downloaded / (1024**3):.1f}GB ({progress:.1f}%)\")\n",
    "                \n",
    "                print(\"‚úÖ ZIP download completed, extracting audio files...\")\n",
    "                temp_zip.seek(0)  # Reset file pointer\n",
    "                \n",
    "                # Process ZIP contents\n",
    "                with zipfile.ZipFile(temp_zip, 'r') as zip_file:\n",
    "                    # Get list of files in ZIP\n",
    "                    zip_files = zip_file.namelist()\n",
    "                    audio_files_in_zip = [f for f in zip_files if f.endswith('.wav')]\n",
    "                    \n",
    "                    print(f\"üìÇ Found {len(audio_files_in_zip)} audio files in ZIP\")\n",
    "                    \n",
    "                    # Process target files found in ZIP\n",
    "                    for zip_audio_path in audio_files_in_zip:\n",
    "                        # Check if this file is in our target list\n",
    "                        audio_filename = Path(zip_audio_path).name\n",
    "                        if not any(audio_filename in target_file for target_file in target_files):\n",
    "                            continue\n",
    "                            \n",
    "                        try:\n",
    "                            print(f\"üéõÔ∏è Processing: {audio_filename}...\")\n",
    "                            \n",
    "                            # Extract audio file to memory\n",
    "                            with zip_file.open(zip_audio_path) as audio_file:\n",
    "                                audio_data = audio_file.read()\n",
    "                            \n",
    "                            # Save to temp file for librosa\n",
    "                            with tempfile.NamedTemporaryFile(suffix='.wav', delete=False) as temp_audio:\n",
    "                                temp_audio.write(audio_data)\n",
    "                                temp_audio_path = temp_audio.name\n",
    "                            \n",
    "                            try:\n",
    "                                # Load audio (limit duration to save memory)\n",
    "                                y, sr = librosa.load(temp_audio_path, sr=22050, duration=60.0)  # 60 seconds\n",
    "                                \n",
    "                                # Generate mel-spectrogram\n",
    "                                mel_spec = librosa.feature.melspectrogram(\n",
    "                                    y=y, sr=sr, n_fft=2048, hop_length=512, n_mels=128\n",
    "                                )\n",
    "                                mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "                                \n",
    "                                # Save spectrogram to Drive\n",
    "                                spec_filename = Path(audio_filename).stem + '_mel.npy'\n",
    "                                spec_path = f'/content/drive/MyDrive/piano_transformer/processed_spectrograms/{spec_filename}'\n",
    "                                \n",
    "                                np.save(spec_path, mel_spec_db)\n",
    "                                print(f\"‚úÖ Saved: {spec_filename} (shape: {mel_spec_db.shape})\")\n",
    "                                processed_count += 1\n",
    "                                \n",
    "                                # Check if we've reached our target (if max_files is set)\n",
    "                                if max_files and processed_count >= max_files:\n",
    "                                    print(f\"üéØ Reached target limit of {processed_count} files\")\n",
    "                                    break\n",
    "                                    \n",
    "                            except Exception as audio_error:\n",
    "                                print(f\"‚ùå Audio processing error: {audio_error}\")\n",
    "                                continue\n",
    "                            finally:\n",
    "                                # Cleanup temp audio file\n",
    "                                if os.path.exists(temp_audio_path):\n",
    "                                    os.remove(temp_audio_path)\n",
    "                                    \n",
    "                        except Exception as extract_error:\n",
    "                            print(f\"‚ùå Extraction error for {zip_audio_path}: {extract_error}\")\n",
    "                            continue\n",
    "                        \n",
    "                        # Storage check periodically\n",
    "                        if processed_count % 10 == 0:\n",
    "                            try:\n",
    "                                storage_info = os.statvfs('/content')\n",
    "                                free_gb = (storage_info.f_bavail * storage_info.f_frsize) / (1024**3)\n",
    "                                print(f\"üíæ Storage: {free_gb:.1f}GB free, {processed_count} files processed\")\n",
    "                            except OSError:\n",
    "                                pass\n",
    "                        \n",
    "                        # Break if we've reached our target\n",
    "                        if max_files and processed_count >= max_files:\n",
    "                            break\n",
    "    \n",
    "    except requests.exceptions.RequestException as download_error:\n",
    "        raise Exception(f\"Failed to download MAESTRO ZIP: {download_error}\")\n",
    "    except zipfile.BadZipFile as zip_error:\n",
    "        raise Exception(f\"Invalid ZIP file: {zip_error}\")\n",
    "    except Exception as general_error:\n",
    "        raise Exception(f\"Processing error: {general_error}\")\n",
    "    \n",
    "    print(f\"\\nüéâ Streaming processing completed!\")\n",
    "    print(f\"‚úÖ Successfully processed: {processed_count} files\")\n",
    "    print(f\"üíæ Spectrograms saved to: /content/drive/MyDrive/piano_transformer/processed_spectrograms/\")\n",
    "    \n",
    "    if processed_count == 0:\n",
    "        raise Exception(\"No files were successfully processed\")\n",
    "    \n",
    "    return processed_count\n",
    "\n",
    "\n",
    "# Run streaming processing with proper error handling\n",
    "try:\n",
    "    # Set max_files=None to process all files, or set a number for testing\n",
    "    # For testing: max_files=50\n",
    "    # For full dataset: max_files=None\n",
    "    num_processed = download_and_process_maestro_streaming(max_files=None)\n",
    "    print(f\"\\n‚úÖ SUCCESS: {num_processed} MAESTRO files processed!\")\n",
    "    print(\"üéØ Ready to proceed with pre-training on processed spectrograms\")\n",
    "        \n",
    "except Exception as main_error:\n",
    "    print(f\"‚ùå Processing failed: {main_error}\")\n",
    "    raise Exception(f\"MAESTRO processing failed: {main_error}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mount_drive"
   },
   "outputs": [],
   "source": [
    "# Cell 4: Quick TPU Verification\n",
    "import jax\n",
    "\n",
    "print(\"üß† Quick TPU check...\")\n",
    "print(f\"JAX backend: {jax.default_backend()}\")\n",
    "print(f\"Device count: {jax.device_count()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ssast_pretraining"
   },
   "outputs": [],
   "source": [
    "# Cell 5: Dataset Setup from Processed Spectrograms\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.append('./src')\n",
    "\n",
    "print(\"üéµ Setting up dataset from processed spectrograms...\")\n",
    "\n",
    "# Check processed spectrograms\n",
    "spec_dir = '/content/drive/MyDrive/piano_transformer/processed_spectrograms'\n",
    "if not os.path.exists(spec_dir):\n",
    "    raise FileNotFoundError(\"Run Cell 3 first to process MAESTRO\")\n",
    "\n",
    "spec_files = [f for f in os.listdir(spec_dir) if f.endswith('_mel.npy')]\n",
    "if len(spec_files) == 0:\n",
    "    raise FileNotFoundError(\"No spectrograms found. Re-run Cell 3\")\n",
    "\n",
    "print(f\"üìä Found {len(spec_files)} processed spectrograms\")\n",
    "\n",
    "# Load metadata\n",
    "metadata_path = '/content/drive/MyDrive/piano_transformer/maestro_metadata.json'\n",
    "with open(metadata_path, 'r') as f:\n",
    "    maestro_metadata = json.load(f)\n",
    "print(\"‚úÖ MAESTRO metadata loaded\")\n",
    "\n",
    "# Dataset class for pre-processed spectrograms\n",
    "class ProcessedSpectrogramDataset:\n",
    "    def __init__(self, spec_dir, metadata):\n",
    "        self.spec_dir = spec_dir\n",
    "        self.metadata = metadata\n",
    "        self.spec_files = [f for f in os.listdir(spec_dir) if f.endswith('_mel.npy')]\n",
    "        self.num_files = len(self.spec_files)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.num_files\n",
    "    \n",
    "    def load_spectrogram(self, idx):\n",
    "        spec_file = self.spec_files[idx]\n",
    "        spec_path = os.path.join(self.spec_dir, spec_file)\n",
    "        return np.load(spec_path)\n",
    "    \n",
    "    def get_batch(self, batch_size=32):\n",
    "        batch_indices = np.random.choice(self.num_files, batch_size, replace=True)\n",
    "        batch_specs = []\n",
    "        \n",
    "        for idx in batch_indices:\n",
    "            spec = self.load_spectrogram(idx)\n",
    "            # Normalize to 128x128\n",
    "            if spec.shape[1] >= 128:\n",
    "                spec = spec[:, :128]\n",
    "            else:\n",
    "                pad_width = 128 - spec.shape[1]\n",
    "                spec = np.pad(spec, ((0, 0), (0, pad_width)), mode='constant')\n",
    "            \n",
    "            batch_specs.append(spec)\n",
    "        \n",
    "        return np.array(batch_specs)\n",
    "\n",
    "# Initialize dataset\n",
    "dataset = ProcessedSpectrogramDataset(spec_dir, maestro_metadata)\n",
    "print(f\"‚úÖ Dataset ready: {len(dataset)} spectrograms\")\n",
    "\n",
    "# Test batch loading\n",
    "test_batch = dataset.get_batch(4)\n",
    "print(f\"‚úÖ Test batch: shape {test_batch.shape}\")\n",
    "print(\"üéØ Ready for SSAST pre-training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "switch_gpu_instructions"
   },
   "outputs": [],
   "source": [
    "# Cell 6: SSAST Pre-training\n",
    "import sys\n",
    "sys.path.append('./src')\n",
    "\n",
    "if 'dataset' not in locals():\n",
    "    raise RuntimeError(\"Run Cell 5 first to load dataset\")\n",
    "\n",
    "print(\"üß† Starting SSAST pre-training...\")\n",
    "print(f\"üìä Training on {len(dataset)} spectrograms\")\n",
    "\n",
    "# Import SSAST trainer\n",
    "try:\n",
    "    from models.ssast_pretraining import SSASTPretrainer\n",
    "    \n",
    "    # Initialize SSAST trainer\n",
    "    trainer = SSASTPretrainer(\n",
    "        model_dim=768,\n",
    "        num_heads=12,\n",
    "        num_layers=12,\n",
    "        patch_size=(16, 16),\n",
    "        num_patches=(8, 8)\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ Using full SSAST implementation\")\n",
    "    \n",
    "    # Run pre-training\n",
    "    results = trainer.pretrain(\n",
    "        dataset=dataset,\n",
    "        num_epochs=100,\n",
    "        batch_size=32,\n",
    "        learning_rate=1e-4,\n",
    "        checkpoint_dir='/content/drive/MyDrive/piano_transformer/checkpoints/ssast',\n",
    "        save_every=10\n",
    "    )\n",
    "    \n",
    "    print(f\"üéâ SSAST pre-training completed!\")\n",
    "    print(f\"üìà Final loss: {results['final_loss']:.4f}\")\n",
    "    \n",
    "except ImportError as e:\n",
    "    raise ImportError(f\"SSAST model not found: {e}. Check src/models/ssast_pretraining.py exists\")\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"Pre-training failed: {e}\")\n",
    "    \n",
    "# Save results\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "training_summary = {\n",
    "    'model': 'SSAST',\n",
    "    'dataset_size': len(dataset),\n",
    "    'final_loss': results['final_loss'],\n",
    "    'epochs': results['epochs'],\n",
    "    'timestamp': datetime.now().isoformat()\n",
    "}\n",
    "\n",
    "with open('/content/drive/MyDrive/piano_transformer/pretraining_results.json', 'w') as f:\n",
    "    json.dump(training_summary, f, indent=2)\n",
    "\n",
    "print(\"üíæ Results saved to Google Drive!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: AST Fine-tuning on PercePiano\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "\n",
    "sys.path.append('./src')\n",
    "\n",
    "try:\n",
    "    from train_ast import train_ast\n",
    "    from datasets.percepiano_dataset import PercepianoDataset\n",
    "    print(\"‚úÖ Fine-tuning modules imported\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Import error: {e}\")\n",
    "    print(\"Check if src/train_ast.py and src/datasets/percepiano_dataset.py exist\")\n",
    "    exit()\n",
    "\n",
    "print(\"üéπ Starting AST fine-tuning on PercePiano dataset...\")\n",
    "print(\"‚è±Ô∏è  Expected duration: ~3 hours on GPU\")\n",
    "\n",
    "# Check for pre-trained model\n",
    "pretrained_path = '/content/drive/MyDrive/piano_transformer/checkpoints/ssast/best_model.pkl'\n",
    "\n",
    "if os.path.exists(pretrained_path):\n",
    "    print(f\"‚úÖ Pre-trained model found: {pretrained_path}\")\n",
    "else:\n",
    "    print(f\"‚ùå Pre-trained model not found at: {pretrained_path}\")\n",
    "    print(\"Available checkpoints:\")\n",
    "    !ls -la /content/drive/MyDrive/piano_transformer/checkpoints/ssast/\n",
    "    # Use the latest checkpoint\n",
    "    checkpoints = !ls /content/drive/MyDrive/piano_transformer/checkpoints/ssast/*.pkl\n",
    "    if checkpoints:\n",
    "        pretrained_path = checkpoints[-1]  # Use latest\n",
    "        print(f\"Using latest checkpoint: {pretrained_path}\")\n",
    "    else:\n",
    "        print(\"No checkpoints found. Running without pre-training.\")\n",
    "        pretrained_path = None\n",
    "\n",
    "# Check PercePiano dataset\n",
    "percepiano_path = './PercePiano'\n",
    "if os.path.exists(percepiano_path):\n",
    "    print(f\"‚úÖ PercePiano dataset found: {percepiano_path}\")\n",
    "else:\n",
    "    print(f\"‚ùå PercePiano dataset not found at: {percepiano_path}\")\n",
    "    exit()\n",
    "\n",
    "# Start fine-tuning\n",
    "print(\"üöÄ Starting fine-tuning...\")\n",
    "results = train_ast(\n",
    "    pretrained_model_path=pretrained_path,\n",
    "    percepiano_path=percepiano_path,\n",
    "    target_correlation=0.7,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    learning_rate=1e-5,  # Lower LR for fine-tuning\n",
    "    checkpoint_dir='/content/drive/MyDrive/piano_transformer/checkpoints/ast_finetuned',\n",
    "    save_every=5\n",
    ")\n",
    "\n",
    "print(\"üéâ Fine-tuning completed! ‚úÖ\")\n",
    "print(f\"Best correlation achieved: {results.get('best_correlation', 'N/A'):.3f}\")\n",
    "\n",
    "# Save fine-tuning results\n",
    "with open('/content/drive/MyDrive/piano_transformer/finetuning_results.json', 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "    \n",
    "print(\"üíæ Fine-tuning results saved to Google Drive!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "verify_gpu"
   },
   "outputs": [],
   "source": [
    "# Cell 9: Final Results and Evaluation\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"üìä Generating final results summary...\")\n",
    "\n",
    "# Load training results\n",
    "pretraining_results = {}\n",
    "finetuning_results = {}\n",
    "\n",
    "# Load pre-training results if available\n",
    "pretraining_path = '/content/drive/MyDrive/piano_transformer/pretraining_results.json'\n",
    "if os.path.exists(pretraining_path):\n",
    "    with open(pretraining_path, 'r') as f:\n",
    "        pretraining_results = json.load(f)\n",
    "    print(\"‚úÖ Pre-training results loaded\")\n",
    "\n",
    "# Load fine-tuning results if available\n",
    "finetuning_path = '/content/drive/MyDrive/piano_transformer/finetuning_results.json'\n",
    "if os.path.exists(finetuning_path):\n",
    "    with open(finetuning_path, 'r') as f:\n",
    "        finetuning_results = json.load(f)\n",
    "    print(\"‚úÖ Fine-tuning results loaded\")\n",
    "\n",
    "# Check final model\n",
    "final_model_path = '/content/drive/MyDrive/piano_transformer/checkpoints/ast_finetuned/best_model.pkl'\n",
    "model_exists = os.path.exists(final_model_path)\n",
    "\n",
    "# Generate comprehensive summary\n",
    "final_summary = {\n",
    "    'experiment_date': datetime.now().isoformat(),\n",
    "    'pipeline': {\n",
    "        'step_1': 'SSAST Pre-training on MAESTRO-v3',\n",
    "        'step_2': 'AST Fine-tuning on PercePiano'\n",
    "    },\n",
    "    'datasets': {\n",
    "        'pretraining': 'MAESTRO-v3 (200+ hours piano audio)',\n",
    "        'finetuning': 'PercePiano (1202 performances, 19 dimensions)'\n",
    "    },\n",
    "    'model_architecture': {\n",
    "        'base': 'Audio Spectrogram Transformer (AST)',\n",
    "        'pretraining': 'Self-Supervised AST (SSAST) with MSPM',\n",
    "        'parameters': '~85M (encoder) + task heads'\n",
    "    },\n",
    "    'results': {\n",
    "        'pretraining': pretraining_results,\n",
    "        'finetuning': finetuning_results,\n",
    "        'final_model_saved': model_exists\n",
    "    },\n",
    "    'target_achieved': finetuning_results.get('best_correlation', 0) >= 0.7\n",
    "}\n",
    "\n",
    "# Save comprehensive results\n",
    "final_results_path = '/content/drive/MyDrive/piano_transformer/FINAL_RESULTS.json'\n",
    "with open(final_results_path, 'w') as f:\n",
    "    json.dump(final_summary, f, indent=2)\n",
    "\n",
    "# Display results\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéâ PIANO PERCEPTION TRANSFORMER - FINAL RESULTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nüìÖ Experiment completed: {datetime.now().strftime('%Y-%m-%d %H:%M')}\")\n",
    "\n",
    "print(\"\\nüèóÔ∏è  Architecture:\")\n",
    "print(\"   ‚Ä¢ Audio Spectrogram Transformer (AST) - 85M parameters\")\n",
    "print(\"   ‚Ä¢ Self-supervised pre-training with MSPM\")\n",
    "print(\"   ‚Ä¢ Multi-task regression for 19 perceptual dimensions\")\n",
    "\n",
    "print(\"\\nüìä Training Pipeline:\")\n",
    "print(\"   1. ‚úÖ SSAST pre-training on MAESTRO-v3 (TPU)\")\n",
    "print(\"   2. ‚úÖ AST fine-tuning on PercePiano (GPU)\")\n",
    "\n",
    "if finetuning_results:\n",
    "    best_corr = finetuning_results.get('best_correlation', 0)\n",
    "    target_met = \"‚úÖ\" if best_corr >= 0.7 else \"‚ùå\"\n",
    "    print(f\"\\nüéØ Performance:\")\n",
    "    print(f\"   ‚Ä¢ Best correlation: {best_corr:.3f}\")\n",
    "    print(f\"   ‚Ä¢ Target >0.7: {target_met}\")\n",
    "\n",
    "print(f\"\\nüíæ All results saved to: {final_results_path}\")\n",
    "print(f\"üìÅ Model checkpoints in: /content/drive/MyDrive/piano_transformer/checkpoints/\")\n",
    "\n",
    "print(\"\\nüéä Training pipeline completed successfully!\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Show file structure\n",
    "print(\"\\nüìÅ Final file structure in Google Drive:\")\n",
    "!ls -la /content/drive/MyDrive/piano_transformer/"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
