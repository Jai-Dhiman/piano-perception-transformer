{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Piano Performance Analysis with CNN Models üéπ\n",
    "\n",
    "**Complete training pipeline for predicting 19 perceptual dimensions using JAX/Flax CNN architectures**\n",
    "\n",
    "This notebook trains deep learning models to analyze piano performances across dimensions like timing, articulation, dynamics, and musical expression using the PercePiano dataset.\n",
    "\n",
    "## Architecture Options\n",
    "- **Standard CNN**: Basic spectral analysis\n",
    "- **Fusion CNN**: Multi-spectral feature fusion (mel + MFCC + chroma)\n",
    "- **Realtime CNN**: Optimized for fast inference\n",
    "\n",
    "## Expected Training Time\n",
    "- GPU (T4): ~2-3 hours for complete training\n",
    "- Dataset: 1,202 performances with 19 perceptual labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup & GPU Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install jax[cuda] flax optax librosa wandb soundfile matplotlib seaborn -q\n",
    "\n",
    "# Verify GPU availability\n",
    "import jax\n",
    "print(f\"JAX devices: {jax.devices()}\")\n",
    "print(f\"JAX backend: {jax.lib.xla_bridge.get_backend().platform}\")\n",
    "\n",
    "gpu_available = len([d for d in jax.devices() if d.device_kind == 'gpu']) > 0\n",
    "print(f\"üöÄ GPU Available: {gpu_available}\")\n",
    "\n",
    "if not gpu_available:\n",
    "    print(\"‚ö†Ô∏è  Enable GPU: Runtime ‚Üí Change runtime type ‚Üí Hardware accelerator ‚Üí GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Upload Project Files\n",
    "\n",
    "Upload your two zip files:\n",
    "1. `piano-analysis-colab.zip` (source code)\n",
    "2. `percepiano-data.zip` (dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "print(\"üìÅ Upload your project files...\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Extract uploaded files\n",
    "for filename in uploaded.keys():\n",
    "    if filename.endswith('.zip'):\n",
    "        with zipfile.ZipFile(filename, 'r') as zip_ref:\n",
    "            zip_ref.extractall('/content/')\n",
    "        print(f\"‚úÖ Extracted {filename}\")\n",
    "        os.remove(filename)  # Clean up\n",
    "\n",
    "# Verify structure\n",
    "print(\"\\nüìÇ Project structure:\")\n",
    "!ls -la /content/src/ | head -10\n",
    "!ls -la /content/data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Setup Project Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add src to Python path\n",
    "import sys\n",
    "sys.path.append('/content/src')\n",
    "\n",
    "# Import all necessary libraries\n",
    "import jax.numpy as jnp\n",
    "import flax.linen as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import wandb\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Import project modules\n",
    "from piano_cnn_jax import get_piano_model\n",
    "from training_pipeline_jax import PianoTrainer, TrainingConfig\n",
    "from dataset_analysis import load_perceptual_labels, PERCEPTUAL_DIMENSIONS\n",
    "from audio_preprocessing import PianoAudioPreprocessor\n",
    "\n",
    "print(\"‚úÖ All imports successful!\")\n",
    "print(f\"üìä Available dimensions: {len(PERCEPTUAL_DIMENSIONS)}\")\n",
    "print(f\"üéµ Sample dimensions: {PERCEPTUAL_DIMENSIONS[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Verification & Quick Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify dataset\n",
    "labels = load_perceptual_labels('/content/data/label_2round_mean_reg_19_with0_rm_highstd0.json')\n",
    "print(f\"‚úÖ Loaded {len(labels)} performances\")\n",
    "\n",
    "# Quick audio verification\n",
    "audio_file = Path('/content/data/Beethoven_WoO80_var27_8bars_3_15.wav')\n",
    "if audio_file.exists():\n",
    "    y, sr = librosa.load(audio_file)\n",
    "    print(f\"‚úÖ Audio sample: {len(y)/sr:.1f}s at {sr}Hz\")\n",
    "    \n",
    "    # Quick visualization\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(y[:sr*2])  # First 2 seconds\n",
    "    plt.title('Waveform (first 2s)')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    S = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=80)\n",
    "    plt.imshow(librosa.power_to_db(S), aspect='auto', origin='lower')\n",
    "    plt.title('Mel Spectrogram')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"‚ùå Audio file not found\")\n",
    "\n",
    "# Dataset statistics\n",
    "all_ratings = np.array([ratings[:-1] for ratings in labels.values()])\n",
    "print(f\"\\nüìà Dataset statistics:\")\n",
    "print(f\"   Shape: {all_ratings.shape}\")\n",
    "print(f\"   Rating range: [{all_ratings.min():.3f}, {all_ratings.max():.3f}]\")\n",
    "print(f\"   Mean rating: {all_ratings.mean():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Configure Training\n",
    "\n",
    "Choose your training configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration - adjust based on your needs\n",
    "config = TrainingConfig(\n",
    "    model_architecture=\"standard\",  # Options: \"standard\", \"fusion\", \"realtime\"\n",
    "    learning_rate=1e-3,\n",
    "    batch_size=16,  # Adjust based on GPU memory\n",
    "    epochs=50,      # Reduce for quick testing, increase for full training\n",
    "    early_stopping_patience=10,\n",
    "    \n",
    "    # Data paths (Colab)\n",
    "    data_path=\"/content/data\",\n",
    "    checkpoint_path=\"/content/checkpoints\",\n",
    "    results_path=\"/content/results\",\n",
    "    \n",
    "    # Audio processing\n",
    "    sample_rate=22050,\n",
    "    n_mels=128,\n",
    "    n_fft=2048,\n",
    "    hop_length=512,\n",
    "    \n",
    "    # Model architecture\n",
    "    base_filters=64,\n",
    "    dropout_rate=0.2\n",
    ")\n",
    "\n",
    "print(\"üîß Training configuration:\")\n",
    "for key, value in config.__dict__.items():\n",
    "    print(f\"   {key}: {value}\")\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(config.checkpoint_path, exist_ok=True)\n",
    "os.makedirs(config.results_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Architecture Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test model architecture before full training\n",
    "print(f\"üß™ Testing {config.model_architecture} architecture...\")\n",
    "\n",
    "model = get_piano_model(\n",
    "    architecture=config.model_architecture,\n",
    "    num_classes=19,\n",
    "    base_filters=config.base_filters,\n",
    "    dropout_rate=config.dropout_rate\n",
    ")\n",
    "\n",
    "# Test with dummy input\n",
    "rng = jax.random.PRNGKey(0)\n",
    "dummy_input = jax.random.normal(rng, (2, 128, 128, 1))\n",
    "\n",
    "params = model.init(rng, dummy_input, training=False)\n",
    "output = model.apply(params, dummy_input, training=False)\n",
    "\n",
    "param_count = sum(x.size for x in jax.tree_util.tree_leaves(params))\n",
    "\n",
    "print(f\"‚úÖ Model test successful:\")\n",
    "print(f\"   Architecture: {config.model_architecture}\")\n",
    "print(f\"   Parameters: {param_count:,}\")\n",
    "print(f\"   Input shape: {dummy_input.shape}\")\n",
    "print(f\"   Output shape: {output.shape}\")\n",
    "print(f\"   Output range: [{output.min():.3f}, {output.max():.3f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Initialize Weights & Biases (Optional)\n",
    "\n",
    "Login to W&B for experiment tracking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Setup wandb for experiment tracking\n",
    "try:\n",
    "    wandb.login()\n",
    "    use_wandb = True\n",
    "    print(\"‚úÖ Weights & Biases connected\")\nexcept:\n",
    "    use_wandb = False\n",
    "    print(\"‚ö†Ô∏è  Skipping W&B - training will continue without logging\")\n",
    "\n",
    "if use_wandb:\n",
    "    wandb.init(\n",
    "        project=\"piano-performance-analysis\",\n",
    "        name=f\"piano-cnn-{config.model_architecture}\",\n",
    "        config=config.__dict__\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Start Training üöÄ\n",
    "\n",
    "This is the main training cell. Expected time: ~2-3 hours with GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize trainer\n",
    "print(\"üöÄ Initializing trainer...\")\n",
    "trainer = PianoTrainer(config)\n",
    "\n",
    "print(f\"üìä Training setup:\")\n",
    "print(f\"   Model parameters: {sum(x.size for x in jax.tree_util.tree_leaves(trainer.state.params)):,}\")\n",
    "print(f\"   Training samples: {len(trainer.train_data['labels'])}\")\n",
    "print(f\"   Validation samples: {len(trainer.val_data['labels'])}\")\n",
    "print(f\"   Test samples: {len(trainer.test_data['labels'])}\")\n",
    "\n",
    "print(\"\\nüéØ Starting training...\")\n",
    "print(\"This may take 2-3 hours. Monitor progress in W&B dashboard if enabled.\")\n",
    "\n",
    "# Start training\n",
    "test_results = trainer.train()\n",
    "\n",
    "print(\"\\nüéâ Training completed!\")\n",
    "print(f\"üìà Final test results:\")\n",
    "for key, value in test_results.items():\n",
    "    if isinstance(value, (int, float)):\n",
    "        print(f\"   {key}: {value:.4f}\")\n",
    "    elif isinstance(value, (list, np.ndarray)):\n",
    "        print(f\"   {key}: mean={np.mean(value):.4f}, std={np.std(value):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Results Analysis & Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and visualize training results\n",
    "results_file = Path(config.results_path) / \"training_results.json\"\n",
    "\n",
    "if results_file.exists():\n",
    "    with open(results_file) as f:\n",
    "        training_results = json.load(f)\n",
    "    \n",
    "    # Create comprehensive visualizations\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Training curves\n",
    "    axes[0, 0].plot(training_results['train_loss'], label='Train', alpha=0.7)\n",
    "    axes[0, 0].plot(training_results['val_loss'], label='Validation', alpha=0.7)\n",
    "    axes[0, 0].set_title('Training Loss')\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('MSE Loss')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Validation correlations over time\n",
    "    if 'val_correlations' in training_results:\n",
    "        val_corrs = np.array(training_results['val_correlations'])\n",
    "        axes[0, 1].plot(val_corrs.mean(axis=1), label='Mean Correlation', color='green')\n",
    "        axes[0, 1].fill_between(range(len(val_corrs)), \n",
    "                               val_corrs.mean(axis=1) - val_corrs.std(axis=1),\n",
    "                               val_corrs.mean(axis=1) + val_corrs.std(axis=1),\n",
    "                               alpha=0.2, color='green')\n",
    "        axes[0, 1].set_title('Validation Correlations')\n",
    "        axes[0, 1].set_xlabel('Epoch')\n",
    "        axes[0, 1].set_ylabel('Correlation')\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Final test correlations by dimension\n",
    "    if 'test_correlations' in test_results:\n",
    "        test_corrs = test_results['test_correlations']\n",
    "        x_pos = np.arange(len(test_corrs))\n",
    "        colors = ['green' if c > 0.5 else 'orange' if c > 0.3 else 'red' for c in test_corrs]\n",
    "        \n",
    "        bars = axes[1, 0].bar(x_pos, test_corrs, color=colors, alpha=0.7)\n",
    "        axes[1, 0].set_title('Test Correlations by Dimension')\n",
    "        axes[1, 0].set_xlabel('Perceptual Dimension')\n",
    "        axes[1, 0].set_ylabel('Correlation')\n",
    "        axes[1, 0].set_xticks(x_pos[::2])  # Show every other label\n",
    "        axes[1, 0].set_xticklabels([PERCEPTUAL_DIMENSIONS[i][:15] for i in x_pos[::2]], \n",
    "                                  rotation=45, ha='right')\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add correlation threshold lines\n",
    "        axes[1, 0].axhline(y=0.5, color='green', linestyle='--', alpha=0.5, label='Good (>0.5)')\n",
    "        axes[1, 0].axhline(y=0.3, color='orange', linestyle='--', alpha=0.5, label='Fair (>0.3)')\n",
    "        axes[1, 0].legend()\n",
    "    \n",
    "    # Model performance summary\n",
    "    axes[1, 1].axis('off')\n",
    "    summary_text = f\"\"\"\n",
    "Model: {config.model_architecture.upper()} CNN\n",
    "Parameters: {sum(x.size for x in jax.tree_util.tree_leaves(trainer.state.params)):,}\n",
    "\n",
    "Dataset:\n",
    "‚Ä¢ Training: {len(trainer.train_data['labels'])} samples\n",
    "‚Ä¢ Validation: {len(trainer.val_data['labels'])} samples  \n",
    "‚Ä¢ Test: {len(trainer.test_data['labels'])} samples\n",
    "\n",
    "Final Performance:\n",
    "‚Ä¢ Test Loss: {test_results.get('test_loss', 'N/A'):.4f}\n",
    "‚Ä¢ Mean Correlation: {np.mean(test_results.get('test_correlations', [0])):.3f}\n",
    "‚Ä¢ Strong Correlations (>0.5): {sum(1 for c in test_results.get('test_correlations', []) if c > 0.5)}/19\n",
    "\n",
    "Training Time: ~{len(training_results.get('train_loss', []))} epochs\n",
    "    \"\"\"\n",
    "    axes[1, 1].text(0.1, 0.9, summary_text, transform=axes[1, 1].transAxes, \n",
    "                    fontsize=10, verticalalignment='top', fontfamily='monospace',\n",
    "                    bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.1))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print top and bottom performing dimensions\n",
    "    if 'test_correlations' in test_results:\n",
    "        correlations = test_results['test_correlations']\n",
    "        sorted_dims = sorted(zip(PERCEPTUAL_DIMENSIONS, correlations), \n",
    "                           key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        print(\"\\nüèÜ Top 5 Best Predicted Dimensions:\")\n",
    "        for i, (dim, corr) in enumerate(sorted_dims[:5]):\n",
    "            print(f\"   {i+1}. {dim}: {corr:.3f}\")\n",
    "        \n",
    "        print(\"\\nüéØ Bottom 5 Dimensions (Need Improvement):\")\n",
    "        for i, (dim, corr) in enumerate(sorted_dims[-5:]):\n",
    "            print(f\"   {i+1}. {dim}: {corr:.3f}\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ùå Training results file not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Save & Download Results\n",
    "\n",
    "Package your results for download:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results package\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "results_name = f\"piano_cnn_results_{config.model_architecture}_{timestamp}\"\n",
    "\n",
    "# Create comprehensive results directory\n",
    "results_dir = f\"/content/{results_name}\"\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "# Copy all important files\n",
    "files_to_save = [\n",
    "    (f\"{config.results_path}/training_results.json\", \"training_results.json\"),\n",
    "    (f\"{config.checkpoint_path}/best_model\", \"model_checkpoint/\"),\n",
    "    (\"/content/Piano_CNN_Training_Complete.ipynb\", \"training_notebook.ipynb\")\n",
    "]\n",
    "\n",
    "for src, dst in files_to_save:\n",
    "    src_path = Path(src)\n",
    "    dst_path = Path(results_dir) / dst\n",
    "    \n",
    "    if src_path.exists():\n",
    "        if src_path.is_dir():\n",
    "            shutil.copytree(src_path, dst_path, dirs_exist_ok=True)\n",
    "        else:\n",
    "            shutil.copy2(src_path, dst_path)\n",
    "        print(f\"‚úÖ Saved: {dst}\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è  Not found: {src}\")\n",
    "\n",
    "# Create summary report\n",
    "summary_report = {\n",
    "    \"model_architecture\": config.model_architecture,\n",
    "    \"training_config\": config.__dict__,\n",
    "    \"final_results\": test_results,\n",
    "    \"training_timestamp\": timestamp,\n",
    "    \"dataset_info\": {\n",
    "        \"total_performances\": len(labels),\n",
    "        \"perceptual_dimensions\": len(PERCEPTUAL_DIMENSIONS),\n",
    "        \"train_samples\": len(trainer.train_data['labels']),\n",
    "        \"val_samples\": len(trainer.val_data['labels']),\n",
    "        \"test_samples\": len(trainer.test_data['labels'])\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(f\"{results_dir}/experiment_summary.json\", 'w') as f:\n",
    "    json.dump(summary_report, f, indent=2)\n",
    "\n",
    "# Create zip file for download\n",
    "shutil.make_archive(results_name, 'zip', '/content', results_name)\n",
    "\n",
    "print(f\"\\nüì¶ Results package created: {results_name}.zip\")\n",
    "print(f\"üìä Package size: {os.path.getsize(f'{results_name}.zip') / 1024 / 1024:.1f} MB\")\n",
    "\n",
    "# Download the results\n",
    "print(\"\\n‚¨áÔ∏è  Downloading results...\")\n",
    "files.download(f\"{results_name}.zip\")\n",
    "\n",
    "print(\"\\nüéâ Training and analysis complete!\")\n",
    "print(\"\\nüìã Next steps for your portfolio:\")\n",
    "print(\"   1. Extract and analyze the downloaded results\")\n",
    "print(\"   2. Document model performance and insights\")\n",
    "print(\"   3. Try different architectures (fusion, realtime)\")\n",
    "print(\"   4. Experiment with hyperparameter tuning\")\n",
    "print(\"   5. Extend to new piano repertoire (Chopin, Liszt)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Quick Model Inference Test\n",
    "\n",
    "Test the trained model on sample audio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test trained model on sample audio\n",
    "if Path(\"/content/data/Beethoven_WoO80_var27_8bars_3_15.wav\").exists():\n",
    "    print(\"üéπ Testing trained model on sample audio...\")\n",
    "    \n",
    "    # Load and process audio\n",
    "    preprocessor = PianoAudioPreprocessor()\n",
    "    audio_result = preprocessor.process_audio_file(\n",
    "        Path(\"/content/data/Beethoven_WoO80_var27_8bars_3_15.wav\"),\n",
    "        \"beethoven_sample\"\n",
    "    )\n",
    "    \n",
    "    # Extract features for prediction\n",
    "    features = np.array([list(audio_result['scalar_features'].values())])\n",
    "    \n",
    "    # Make prediction (simplified - normally you'd use spectrograms)\n",
    "    print(f\"\\nüéØ Sample prediction (using audio features):\")\n",
    "    print(f\"   Audio duration: {audio_result['duration']:.1f}s\")\n",
    "    print(f\"   Features extracted: {len(audio_result['scalar_features'])}\")\n",
    "    \n",
    "    # Show some interesting correlations from your analysis\n",
    "    feature_names = list(audio_result['scalar_features'].keys())\n",
    "    for i, (dim, feature) in enumerate(zip(PERCEPTUAL_DIMENSIONS[:5], feature_names[:5])):\n",
    "        print(f\"   {dim}: {features[0][i]:.3f} (audio feature: {feature})\")\n",
    "    \n",
    "    print(\"\\nüìù Note: Full CNN prediction would use spectrograms as input\")\n",
    "    print(\"    This demonstrates the feature extraction pipeline\")\nelse:\n",
    "    print(\"‚ö†Ô∏è  Sample audio not found for inference test\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
