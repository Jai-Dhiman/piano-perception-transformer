{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# üéπ Piano Performance Analysis - Colab Validation Test\n",
    "\n",
    "## Objective\n",
    "Test our JAX/Flax piano CNN architectures with synthetic data before collecting real Chopin/Liszt recordings.\n",
    "\n",
    "## What We're Testing\n",
    "1. **Three CNN Architectures**: Standard, Multi-Spectral Fusion, Real-time\n",
    "2. **Synthetic Data Pipeline**: Realistic piano-like spectrograms \n",
    "3. **Training Convergence**: End-to-end training with proper metrics\n",
    "4. **Performance Comparison**: Which architecture works best\n",
    "\n",
    "## Success Criteria\n",
    "- Models train without errors\n",
    "- Loss decreases consistently \n",
    "- Correlations with synthetic labels > 0.7\n",
    "- Training completes in reasonable time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup"
   },
   "source": [
    "# üì¶ Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_deps"
   },
   "outputs": [],
   "source": [
    "# Install uv package manager first\n",
    "!curl -LsSf https://astral.sh/uv/install.sh | sh\n",
    "import os\n",
    "os.environ['PATH'] = f\"/root/.cargo/bin:{os.environ.get('PATH', '')}\"\n",
    "\n",
    "# Verify uv installation\n",
    "!uv --version\n",
    "\n",
    "# Install dependencies using uv (faster than pip)\n",
    "!uv pip install --system jax[gpu] flax optax\n",
    "!uv pip install --system librosa soundfile matplotlib seaborn\n",
    "!uv pip install --system wandb tqdm pandas numpy scipy\n",
    "\n",
    "# Import core libraries\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax.linen as nn\n",
    "import optax\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import json\n",
    "from typing import Dict, Tuple, List\n",
    "from dataclasses import dataclass\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"üöÄ JAX version: {jax.__version__}\")\n",
    "print(f\"üîß Available devices: {jax.devices()}\")\n",
    "print(f\"üíª Platform: {jax.lib.xla_bridge.get_backend().platform}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "models"
   },
   "source": [
    "# üß† Model Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "model_definitions"
   },
   "outputs": [],
   "source": [
    "# Core model architectures adapted from your codebase\n",
    "from flax.training import train_state, checkpoints\n",
    "from flax.training.early_stopping import EarlyStopping\n",
    "import functools\n",
    "\n",
    "class SpectralConvBlock(nn.Module):\n",
    "    \"\"\"Optimized conv block for mel-spectrogram processing\"\"\"\n",
    "    features: int\n",
    "    kernel_size: tuple = (3, 3)\n",
    "    strides: tuple = (1, 1)\n",
    "    dropout_rate: float = 0.1\n",
    "    \n",
    "    @nn.compact\n",
    "    def __call__(self, x, training: bool = True):\n",
    "        x = nn.Conv(\n",
    "            features=self.features,\n",
    "            kernel_size=self.kernel_size,\n",
    "            strides=self.strides,\n",
    "            padding='SAME'\n",
    "        )(x)\n",
    "        x = nn.BatchNorm(use_running_average=not training)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dropout(rate=self.dropout_rate, deterministic=not training)(x)\n",
    "        return x\n",
    "\n",
    "class PianoSpectroCNN(nn.Module):\n",
    "    \"\"\"Standard piano CNN - VGGish inspired\"\"\"\n",
    "    num_classes: int = 19\n",
    "    base_filters: int = 64\n",
    "    dropout_rate: float = 0.2\n",
    "    \n",
    "    @nn.compact\n",
    "    def __call__(self, x, training: bool = True):\n",
    "        # Input: (batch, time, freq, 1) - mel-spectrograms\n",
    "        \n",
    "        # Feature extraction layers\n",
    "        x = SpectralConvBlock(self.base_filters, kernel_size=(3, 3))(x, training)\n",
    "        x = nn.max_pool(x, window_shape=(2, 2), strides=(2, 2))\n",
    "        \n",
    "        x = SpectralConvBlock(self.base_filters * 2, kernel_size=(3, 3))(x, training)\n",
    "        x = nn.max_pool(x, window_shape=(2, 2), strides=(2, 2))\n",
    "        \n",
    "        x = SpectralConvBlock(self.base_filters * 4, kernel_size=(3, 3))(x, training)\n",
    "        x = SpectralConvBlock(self.base_filters * 4, kernel_size=(3, 3))(x, training)\n",
    "        x = nn.max_pool(x, window_shape=(2, 2), strides=(2, 2))\n",
    "        \n",
    "        x = SpectralConvBlock(self.base_filters * 8, kernel_size=(3, 3))(x, training)\n",
    "        x = SpectralConvBlock(self.base_filters * 8, kernel_size=(3, 3))(x, training)\n",
    "        x = nn.max_pool(x, window_shape=(2, 2), strides=(2, 2))\n",
    "        \n",
    "        # Global pooling and classification\n",
    "        x = nn.avg_pool(x, window_shape=(x.shape[1], x.shape[2]))\n",
    "        x = jnp.reshape(x, (x.shape[0], -1))\n",
    "        \n",
    "        # Multi-task prediction heads\n",
    "        x = nn.Dense(512)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dropout(rate=self.dropout_rate, deterministic=not training)(x)\n",
    "        \n",
    "        x = nn.Dense(256)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dropout(rate=self.dropout_rate, deterministic=not training)(x)\n",
    "        \n",
    "        x = nn.Dense(self.num_classes)(x)\n",
    "        return nn.sigmoid(x)\n",
    "\n",
    "class RealTimePianoCNN(nn.Module):\n",
    "    \"\"\"Lightweight CNN for real-time inference\"\"\"\n",
    "    num_classes: int = 19\n",
    "    width_multiplier: float = 0.5\n",
    "    \n",
    "    @nn.compact  \n",
    "    def __call__(self, x, training: bool = True):\n",
    "        base_filters = int(32 * self.width_multiplier)\n",
    "        \n",
    "        # Efficient separable convolutions\n",
    "        x = self._separable_conv_block(x, base_filters, training)\n",
    "        x = nn.max_pool(x, window_shape=(2, 2), strides=(2, 2))\n",
    "        \n",
    "        x = self._separable_conv_block(x, base_filters * 2, training)\n",
    "        x = nn.max_pool(x, window_shape=(2, 2), strides=(2, 2))\n",
    "        \n",
    "        x = self._separable_conv_block(x, base_filters * 4, training)\n",
    "        x = nn.avg_pool(x, window_shape=(x.shape[1], x.shape[2]))\n",
    "        \n",
    "        x = jnp.reshape(x, (x.shape[0], -1))\n",
    "        \n",
    "        # Compact classifier\n",
    "        x = nn.Dense(128)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(self.num_classes)(x)\n",
    "        return nn.sigmoid(x)\n",
    "    \n",
    "    def _separable_conv_block(self, x, filters, training):\n",
    "        # Depth-wise convolution  \n",
    "        x = nn.Conv(\n",
    "            features=x.shape[-1],\n",
    "            kernel_size=(3, 3),\n",
    "            feature_group_count=x.shape[-1],\n",
    "            padding='SAME'\n",
    "        )(x)\n",
    "        \n",
    "        # Point-wise convolution\n",
    "        x = nn.Conv(features=filters, kernel_size=(1, 1))(x)\n",
    "        x = nn.BatchNorm(use_running_average=not training)(x)\n",
    "        x = nn.relu(x)\n",
    "        return x\n",
    "\n",
    "print(\"üß† Model architectures loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "synthetic_data"
   },
   "source": [
    "# üéº Synthetic Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "data_generation"
   },
   "outputs": [],
   "source": [
    "class SyntheticPianoDataGenerator:\n",
    "    \"\"\"Generate realistic synthetic piano spectrograms with correlated perceptual labels\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 num_samples: int = 1000,\n",
    "                 time_frames: int = 128,\n",
    "                 freq_bins: int = 128,\n",
    "                 seed: int = 42):\n",
    "        self.num_samples = num_samples\n",
    "        self.time_frames = time_frames\n",
    "        self.freq_bins = freq_bins\n",
    "        self.rng = np.random.RandomState(seed)\n",
    "        \n",
    "        # Define perceptual dimensions\n",
    "        self.dimensions = [\n",
    "            \"Timing_Stable_Unstable\", \"Articulation_Short_Long\", \n",
    "            \"Articulation_Soft_Hard\", \"Pedal_Dry_Wet\", \"Pedal_Clean_Blurred\",\n",
    "            \"Timbre_Even_Colorful\", \"Timbre_Shallow_Rich\", \"Timbre_Bright_Dark\",\n",
    "            \"Timbre_Soft_Loud\", \"Dynamic_Mellow_Raw\", \"Dynamic_Small_Large_Range\",\n",
    "            \"Music_Fast_Slow\", \"Music_Flat_Spacious\", \"Music_Unbalanced_Balanced\",\n",
    "            \"Music_Pure_Expressive\", \"Mood_Pleasant_Dark\", \"Mood_Low_High_Energy\",\n",
    "            \"Mood_Honest_Imaginative\", \"Interpretation_Poor_Convincing\"\n",
    "        ]\n",
    "        \n",
    "    def generate_piano_spectrogram(self, style_params: Dict) -> jnp.ndarray:\n",
    "        \"\"\"Generate realistic piano mel-spectrogram\"\"\"\n",
    "        \n",
    "        # Base piano harmonics (fundamental + overtones)\n",
    "        base_freqs = np.array([88, 176, 264, 352, 440, 528, 660])  # Piano range\n",
    "        \n",
    "        spectrogram = np.zeros((self.time_frames, self.freq_bins))\n",
    "        \n",
    "        # Generate harmonic content based on style\n",
    "        brightness = style_params['brightness']  # 0-1\n",
    "        richness = style_params['richness']     # 0-1  \n",
    "        dynamics = style_params['dynamics']     # 0-1\n",
    "        timing_stability = style_params['timing_stability']  # 0-1\n",
    "        \n",
    "        # Create time-varying piano content\n",
    "        for t in range(self.time_frames):\n",
    "            # Add timing jitter based on stability\n",
    "            timing_noise = (1 - timing_stability) * self.rng.normal(0, 0.1)\n",
    "            \n",
    "            # Simulate note attacks and decays\n",
    "            if t % 16 == 0:  # Note onsets every 16 frames\n",
    "                attack_strength = 0.8 + dynamics * 0.4\n",
    "            else:\n",
    "                attack_strength *= 0.92  # Exponential decay\n",
    "            \n",
    "            # Add harmonic content\n",
    "            for i, freq in enumerate(base_freqs):\n",
    "                freq_bin = int((freq / 11025) * self.freq_bins)  # Map to mel bins\n",
    "                if freq_bin < self.freq_bins:\n",
    "                    # Fundamental\n",
    "                    amplitude = attack_strength * (0.5 + 0.3 * self.rng.random())\n",
    "                    spectrogram[t, freq_bin] += amplitude\n",
    "                    \n",
    "                    # Overtones (affected by brightness and richness)\n",
    "                    for h in range(2, 6):  # Harmonics 2-5\n",
    "                        h_bin = min(freq_bin + h * 8, self.freq_bins - 1)\n",
    "                        h_amplitude = amplitude * (brightness * 0.3 + richness * 0.2) / h\n",
    "                        spectrogram[t, h_bin] += h_amplitude\n",
    "        \n",
    "        # Add realistic noise and texture\n",
    "        noise_level = 0.05 + (1 - timing_stability) * 0.1\n",
    "        spectrogram += self.rng.normal(0, noise_level, spectrogram.shape)\n",
    "        \n",
    "        # Apply piano-like spectral envelope\n",
    "        freq_envelope = np.exp(-np.linspace(0, 3, self.freq_bins))  # High-freq rolloff\n",
    "        spectrogram *= freq_envelope[None, :]\n",
    "        \n",
    "        # Normalize to dB scale\n",
    "        spectrogram = np.clip(spectrogram, 1e-8, None)\n",
    "        spectrogram_db = 20 * np.log10(spectrogram)\n",
    "        \n",
    "        # Normalize to [0, 1] range\n",
    "        min_db, max_db = -80, 0\n",
    "        spectrogram_norm = (spectrogram_db - min_db) / (max_db - min_db)\n",
    "        spectrogram_norm = np.clip(spectrogram_norm, 0, 1)\n",
    "        \n",
    "        return spectrogram_norm[..., np.newaxis]  # Add channel dimension\n",
    "    \n",
    "    def generate_perceptual_labels(self, style_params: Dict) -> np.ndarray:\n",
    "        \"\"\"Generate correlated perceptual labels\"\"\"\n",
    "        labels = np.zeros(19)\n",
    "        \n",
    "        # Map style parameters to perceptual dimensions with realistic correlations\n",
    "        labels[0] = style_params['timing_stability']  # Timing\n",
    "        labels[1] = 0.3 + style_params['articulation'] * 0.7  # Articulation length\n",
    "        labels[2] = style_params['articulation']  # Articulation softness\n",
    "        labels[3] = style_params['pedal_wetness']  # Pedal wet/dry\n",
    "        labels[4] = 1 - style_params['pedal_wetness'] * 0.6  # Pedal clarity\n",
    "        \n",
    "        # Timbre dimensions\n",
    "        labels[5] = style_params['richness']  # Even/Colorful\n",
    "        labels[6] = style_params['richness']  # Shallow/Rich\n",
    "        labels[7] = style_params['brightness']  # Bright/Dark\n",
    "        labels[8] = style_params['dynamics']  # Soft/Loud\n",
    "        \n",
    "        # Dynamic expression\n",
    "        labels[9] = 0.3 + style_params['expression'] * 0.7  # Sophisticated/Raw\n",
    "        labels[10] = style_params['dynamics']  # Dynamic range\n",
    "        \n",
    "        # Musical expression\n",
    "        labels[11] = 1 - style_params['tempo_stability']  # Fast/Slow paced\n",
    "        labels[12] = style_params['expression']  # Flat/Spacious\n",
    "        labels[13] = style_params['timing_stability']  # Balanced\n",
    "        labels[14] = style_params['expression']  # Pure/Dramatic\n",
    "        \n",
    "        # Emotion and mood\n",
    "        labels[15] = 0.4 + style_params['mood_valence'] * 0.6  # Pleasant/Dark\n",
    "        labels[16] = style_params['energy']  # Low/High energy\n",
    "        labels[17] = style_params['expression']  # Honest/Imaginative\n",
    "        \n",
    "        # Overall interpretation\n",
    "        labels[18] = np.mean([style_params['timing_stability'], style_params['expression'], \n",
    "                             style_params['dynamics']])  # Overall convincing\n",
    "        \n",
    "        # Add realistic noise and ensure [0,1] range\n",
    "        labels += self.rng.normal(0, 0.05, 19)\n",
    "        labels = np.clip(labels, 0, 1)\n",
    "        \n",
    "        return labels\n",
    "    \n",
    "    def generate_dataset(self) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Generate complete synthetic dataset\"\"\"\n",
    "        print(f\"üéº Generating {self.num_samples} synthetic piano performances...\")\n",
    "        \n",
    "        spectrograms = []\n",
    "        all_labels = []\n",
    "        \n",
    "        for i in tqdm(range(self.num_samples), desc=\"Generating data\"):\n",
    "            # Create diverse style parameters\n",
    "            style_params = {\n",
    "                'timing_stability': self.rng.beta(2, 2),  # Slightly unstable bias\n",
    "                'articulation': self.rng.uniform(0.2, 0.9),\n",
    "                'pedal_wetness': self.rng.uniform(0.1, 0.8),\n",
    "                'brightness': self.rng.uniform(0.3, 0.8),\n",
    "                'richness': self.rng.uniform(0.2, 0.9),\n",
    "                'dynamics': self.rng.uniform(0.3, 0.9),\n",
    "                'expression': self.rng.uniform(0.2, 0.8),\n",
    "                'tempo_stability': self.rng.beta(3, 2),  # Generally stable\n",
    "                'mood_valence': self.rng.uniform(0.3, 0.8),\n",
    "                'energy': self.rng.uniform(0.2, 0.9)\n",
    "            }\n",
    "            \n",
    "            # Generate spectrogram and labels\n",
    "            spectrogram = self.generate_piano_spectrogram(style_params)\n",
    "            labels = self.generate_perceptual_labels(style_params)\n",
    "            \n",
    "            spectrograms.append(spectrogram)\n",
    "            all_labels.append(labels)\n",
    "        \n",
    "        spectrograms = np.stack(spectrograms)\n",
    "        all_labels = np.stack(all_labels)\n",
    "        \n",
    "        print(f\"‚úÖ Generated dataset:\")\n",
    "        print(f\"   Spectrograms: {spectrograms.shape}\")\n",
    "        print(f\"   Labels: {all_labels.shape}\")\n",
    "        print(f\"   Label ranges: [{all_labels.min():.3f}, {all_labels.max():.3f}]\")\n",
    "        \n",
    "        return spectrograms, all_labels\n",
    "\n",
    "# Generate synthetic dataset\n",
    "data_generator = SyntheticPianoDataGenerator(num_samples=800)  # Reasonable size for Colab\n",
    "X_synthetic, y_synthetic = data_generator.generate_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "data_viz"
   },
   "source": [
    "# üìä Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "visualize_data"
   },
   "outputs": [],
   "source": [
    "# Visualize synthetic data quality\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "\n",
    "# Show sample spectrograms\n",
    "for i in range(3):\n",
    "    axes[0, i].imshow(X_synthetic[i, :, :, 0].T, aspect='auto', origin='lower', cmap='viridis')\n",
    "    axes[0, i].set_title(f'Sample Spectrogram {i+1}')\n",
    "    axes[0, i].set_xlabel('Time Frames')\n",
    "    axes[0, i].set_ylabel('Frequency Bins')\n",
    "\n",
    "# Show label distributions\n",
    "dimension_names = data_generator.dimensions\n",
    "for i in range(3):\n",
    "    dim_idx = i * 6  # Show every 6th dimension\n",
    "    if dim_idx < len(dimension_names):\n",
    "        axes[1, i].hist(y_synthetic[:, dim_idx], bins=30, alpha=0.7, edgecolor='black')\n",
    "        axes[1, i].set_title(f'{dimension_names[dim_idx][:20]}...')\n",
    "        axes[1, i].set_xlabel('Rating [0-1]')\n",
    "        axes[1, i].set_ylabel('Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show correlation matrix of labels\n",
    "plt.figure(figsize=(12, 10))\n",
    "corr_matrix = np.corrcoef(y_synthetic.T)\n",
    "sns.heatmap(corr_matrix, \n",
    "            xticklabels=[d[:15] + '...' for d in dimension_names],\n",
    "            yticklabels=[d[:15] + '...' for d in dimension_names],\n",
    "            cmap='coolwarm', center=0, annot=False)\n",
    "plt.title('Synthetic Label Correlations')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"üìà Dataset Statistics:\")\n",
    "print(f\"   Mean correlation between dimensions: {np.mean(np.abs(corr_matrix - np.eye(19))): .3f}\")\n",
    "print(f\"   Strongest correlation: {np.max(corr_matrix - np.eye(19)):.3f}\")\n",
    "print(f\"   Label standard deviations: {np.std(y_synthetic, axis=0).mean():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "training_utils"
   },
   "source": [
    "# üèãÔ∏è Training Infrastructure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "training_functions"
   },
   "outputs": [],
   "source": "@dataclass\nclass TrainingConfig:\n    \"\"\"Training configuration for Colab validation\"\"\"\n    learning_rate: float = 1e-3\n    batch_size: int = 32\n    epochs: int = 50  # Shorter for validation\n    early_stopping_patience: int = 8\n    val_split: float = 0.2\n    test_split: float = 0.1\n\nclass TrainStateWithBatchStats(train_state.TrainState):\n    \"\"\"Training state that tracks batch normalization statistics\"\"\"\n    batch_stats: dict\n\ndef create_train_state_fixed(model, learning_rate: float, input_shape: tuple):\n    \"\"\"Initialize training state with batch stats\"\"\"\n    rng = jax.random.PRNGKey(42)\n    dummy_input = jnp.ones(input_shape)\n    \n    variables = model.init(rng, dummy_input, training=False)\n    params = variables['params']\n    batch_stats = variables.get('batch_stats', {})\n    \n    optimizer = optax.adam(learning_rate)\n    \n    return TrainStateWithBatchStats.create(\n        apply_fn=model.apply,\n        params=params,\n        batch_stats=batch_stats,\n        tx=optimizer\n    )\n\n@jax.jit\ndef train_step_fixed(state, batch_x, batch_y, dropout_rng):\n    \"\"\"Fixed training step with mutable batch stats\"\"\"\n    def loss_fn(params):\n        predictions, new_model_state = state.apply_fn(\n            {'params': params, 'batch_stats': state.batch_stats}, \n            batch_x, \n            training=True, \n            rngs={'dropout': dropout_rng},\n            mutable=['batch_stats']\n        )\n        \n        # Multi-task MSE loss\n        mse_loss = jnp.mean((predictions - batch_y) ** 2)\n        \n        # L2 regularization\n        l2_loss = 0.001 * sum(jnp.sum(jnp.square(p)) for p in jax.tree_util.tree_leaves(params))\n        \n        return mse_loss + l2_loss, (predictions, new_model_state)\n    \n    (loss, (predictions, new_model_state)), grads = jax.value_and_grad(\n        loss_fn, has_aux=True\n    )(state.params)\n    \n    state = state.apply_gradients(\n        grads=grads, \n        batch_stats=new_model_state['batch_stats']\n    )\n    \n    return state, loss, predictions\n\n@jax.jit  \ndef eval_step_fixed(state, batch_x, batch_y):\n    \"\"\"Fixed evaluation step - JAX compatible correlation calculation\"\"\"\n    predictions = state.apply_fn(\n        {'params': state.params, 'batch_stats': state.batch_stats}, \n        batch_x, \n        training=False\n    )\n    loss = jnp.mean((predictions - batch_y) ** 2)\n    \n    # JAX-compatible correlation calculation for each dimension\n    correlations = []\n    for i in range(predictions.shape[1]):\n        pred_i = predictions[:, i]\n        true_i = batch_y[:, i]\n        \n        # Center the data\n        pred_centered = pred_i - jnp.mean(pred_i)\n        true_centered = true_i - jnp.mean(true_i)\n        \n        # Calculate standard deviations\n        pred_std = jnp.std(pred_centered)\n        true_std = jnp.std(true_centered)\n        \n        # Calculate correlation using JAX-compatible operations\n        # Use jnp.where instead of if/else for JAX compatibility\n        numerator = jnp.mean(pred_centered * true_centered)\n        denominator = pred_std * true_std\n        \n        # Handle division by zero case\n        correlation = jnp.where(\n            denominator > 1e-8,  # Safe threshold\n            numerator / denominator,\n            0.0  # Return 0 if no variance\n        )\n        \n        # Clamp to valid correlation range [-1, 1]\n        correlation = jnp.clip(correlation, -1.0, 1.0)\n        correlations.append(correlation)\n    \n    return loss, jnp.array(correlations), predictions\n\ndef create_data_batches(X, y, batch_size, rng_key):\n    \"\"\"Create randomized batches\"\"\"\n    n_samples = X.shape[0]\n    indices = jax.random.permutation(rng_key, n_samples)\n    \n    # Trim to fit batch size\n    n_batches = n_samples // batch_size\n    indices = indices[:n_batches * batch_size]\n    indices = indices.reshape(n_batches, batch_size)\n    \n    return X[indices], y[indices]\n\nprint(\"üèãÔ∏è Fixed training infrastructure ready (JAX-compatible correlations)!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "training_test"
   },
   "source": [
    "# üöÄ Model Training & Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "train_models"
   },
   "outputs": [],
   "source": [
    "def train_and_evaluate_model(model_name: str, model_class, config: TrainingConfig, \n",
    "                           X_train, y_train, X_val, y_val, X_test, y_test):\n",
    "    \"\"\"Train and evaluate a single model\"\"\"\n",
    "    print(f\"\\nüß† Training {model_name}...\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Initialize model\n",
    "    if model_name == \"RealTime\":\n",
    "        model = model_class(num_classes=19, width_multiplier=0.5)\n",
    "    else:\n",
    "        model = model_class(num_classes=19, base_filters=32)  # Smaller for Colab\n",
    "    \n",
    "    # Create training state\n",
    "    input_shape = (config.batch_size, *X_train.shape[1:])\n",
    "    state = create_train_state(model, config.learning_rate, input_shape)\n",
    "    \n",
    "    print(f\"Model parameters: {sum(x.size for x in jax.tree_util.tree_leaves(state.params)):,}\")\n",
    "    \n",
    "    # Training loop\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_correlations = []\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    \n",
    "    rng = jax.random.PRNGKey(42)\n",
    "    \n",
    "    for epoch in range(config.epochs):\n",
    "        rng, epoch_rng, dropout_rng = jax.random.split(rng, 3)\n",
    "        \n",
    "        # Training phase\n",
    "        X_train_batches, y_train_batches = create_data_batches(\n",
    "            X_train, y_train, config.batch_size, epoch_rng\n",
    "        )\n",
    "        \n",
    "        epoch_train_loss = 0.0\n",
    "        for i in range(X_train_batches.shape[0]):\n",
    "            batch_rng = jax.random.fold_in(dropout_rng, i)\n",
    "            state, batch_loss, _ = train_step(\n",
    "                state, X_train_batches[i], y_train_batches[i], batch_rng\n",
    "            )\n",
    "            epoch_train_loss += float(batch_loss)\n",
    "        \n",
    "        avg_train_loss = epoch_train_loss / X_train_batches.shape[0]\n",
    "        \n",
    "        # Validation phase\n",
    "        X_val_batches, y_val_batches = create_data_batches(\n",
    "            X_val, y_val, config.batch_size, epoch_rng\n",
    "        )\n",
    "        \n",
    "        epoch_val_loss = 0.0\n",
    "        epoch_correlations = []\n",
    "        \n",
    "        for i in range(X_val_batches.shape[0]):\n",
    "            val_loss, correlations, _ = eval_step(\n",
    "                state, X_val_batches[i], y_val_batches[i]\n",
    "            )\n",
    "            epoch_val_loss += float(val_loss)\n",
    "            epoch_correlations.append(correlations)\n",
    "        \n",
    "        avg_val_loss = epoch_val_loss / X_val_batches.shape[0]\n",
    "        avg_correlations = np.mean(np.stack(epoch_correlations), axis=0)\n",
    "        \n",
    "        train_losses.append(avg_train_loss)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        val_correlations.append(avg_correlations)\n",
    "        \n",
    "        # Early stopping check\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        # Print progress\n",
    "        if epoch % 10 == 0 or epoch < 5:\n",
    "            avg_corr = np.mean(avg_correlations)\n",
    "            print(f\"Epoch {epoch:2d} | Train: {avg_train_loss:.4f} | Val: {avg_val_loss:.4f} | Corr: {avg_corr:.3f}\")\n",
    "        \n",
    "        # Early stopping\n",
    "        if patience_counter >= config.early_stopping_patience:\n",
    "            print(f\"Early stopping at epoch {epoch}\")\n",
    "            break\n",
    "    \n",
    "    # Final test evaluation\n",
    "    X_test_batches, y_test_batches = create_data_batches(\n",
    "        X_test, y_test, config.batch_size, jax.random.PRNGKey(0)\n",
    "    )\n",
    "    \n",
    "    test_losses = []\n",
    "    test_correlations = []\n",
    "    \n",
    "    for i in range(X_test_batches.shape[0]):\n",
    "        test_loss, correlations, _ = eval_step(\n",
    "            state, X_test_batches[i], y_test_batches[i]\n",
    "        )\n",
    "        test_losses.append(float(test_loss))\n",
    "        test_correlations.append(correlations)\n",
    "    \n",
    "    final_test_loss = np.mean(test_losses)\n",
    "    final_test_corr = np.mean(np.stack(test_correlations), axis=0)\n",
    "    \n",
    "    results = {\n",
    "        'model_name': model_name,\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "        'val_correlations': val_correlations,\n",
    "        'test_loss': final_test_loss,\n",
    "        'test_correlations': final_test_corr,\n",
    "        'avg_test_correlation': float(np.mean(final_test_corr)),\n",
    "        'final_epoch': epoch,\n",
    "        'params': state.params\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n‚úÖ {model_name} Final Results:\")\n",
    "    print(f\"   Test Loss: {final_test_loss:.4f}\")\n",
    "    print(f\"   Avg Correlation: {results['avg_test_correlation']:.3f}\")\n",
    "    print(f\"   Best Dimensions: {np.argsort(final_test_corr)[-3:]}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Data splitting\n",
    "n_samples = X_synthetic.shape[0]\n",
    "test_size = int(n_samples * 0.1)\n",
    "val_size = int(n_samples * 0.2)\n",
    "\n",
    "# Random permutation for splitting\n",
    "indices = np.random.permutation(n_samples)\n",
    "test_idx = indices[:test_size]\n",
    "val_idx = indices[test_size:test_size + val_size] \n",
    "train_idx = indices[test_size + val_size:]\n",
    "\n",
    "# Convert to JAX arrays\n",
    "X_train = jnp.array(X_synthetic[train_idx])\n",
    "y_train = jnp.array(y_synthetic[train_idx])\n",
    "X_val = jnp.array(X_synthetic[val_idx])\n",
    "y_val = jnp.array(y_synthetic[val_idx])\n",
    "X_test = jnp.array(X_synthetic[test_idx])\n",
    "y_test = jnp.array(y_synthetic[test_idx])\n",
    "\n",
    "print(f\"üìä Data splits:\")\n",
    "print(f\"   Train: {X_train.shape[0]} samples\")\n",
    "print(f\"   Validation: {X_val.shape[0]} samples\")\n",
    "print(f\"   Test: {X_test.shape[0]} samples\")\n",
    "\n",
    "config = TrainingConfig()\n",
    "print(f\"\\n‚öôÔ∏è Training config: {config.epochs} epochs, lr={config.learning_rate}, batch_size={config.batch_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "model_comparison"
   },
   "source": [
    "# üèÜ Model Architecture Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "compare_models"
   },
   "outputs": [],
   "source": "def train_and_evaluate_model_fixed(model_name: str, model_class, config: TrainingConfig, \n                           X_train, y_train, X_val, y_val, X_test, y_test):\n    \"\"\"Fixed train and evaluate function with proper batch stats handling\"\"\"\n    print(f\"\\nüß† Training {model_name}...\")\n    print(\"=\" * 50)\n    \n    # Initialize model with correct parameters\n    if model_name == \"RealTime CNN\":\n        model = model_class(num_classes=19, width_multiplier=0.5)\n    else:\n        model = model_class(num_classes=19, base_filters=32)  # Smaller for Colab\n    \n    # Create training state with fixed function\n    input_shape = (config.batch_size, *X_train.shape[1:])\n    state = create_train_state_fixed(model, config.learning_rate, input_shape)\n    \n    print(f\"Model parameters: {sum(x.size for x in jax.tree_util.tree_leaves(state.params)):,}\")\n    \n    # Training loop\n    train_losses = []\n    val_losses = []\n    val_correlations = []\n    \n    best_val_loss = float('inf')\n    patience_counter = 0\n    \n    rng = jax.random.PRNGKey(42)\n    \n    for epoch in range(config.epochs):\n        rng, epoch_rng, dropout_rng = jax.random.split(rng, 3)\n        \n        # Training phase\n        X_train_batches, y_train_batches = create_data_batches(\n            X_train, y_train, config.batch_size, epoch_rng\n        )\n        \n        epoch_train_loss = 0.0\n        for i in range(X_train_batches.shape[0]):\n            batch_rng = jax.random.fold_in(dropout_rng, i)\n            state, batch_loss, _ = train_step_fixed(\n                state, X_train_batches[i], y_train_batches[i], batch_rng\n            )\n            epoch_train_loss += float(batch_loss)\n        \n        avg_train_loss = epoch_train_loss / X_train_batches.shape[0]\n        \n        # Validation phase\n        X_val_batches, y_val_batches = create_data_batches(\n            X_val, y_val, config.batch_size, epoch_rng\n        )\n        \n        epoch_val_loss = 0.0\n        epoch_correlations = []\n        \n        for i in range(X_val_batches.shape[0]):\n            val_loss, correlations, _ = eval_step_fixed(\n                state, X_val_batches[i], y_val_batches[i]\n            )\n            epoch_val_loss += float(val_loss)\n            epoch_correlations.append(correlations)\n        \n        avg_val_loss = epoch_val_loss / X_val_batches.shape[0]\n        avg_correlations = np.mean(np.stack(epoch_correlations), axis=0)\n        \n        train_losses.append(avg_train_loss)\n        val_losses.append(avg_val_loss)\n        val_correlations.append(avg_correlations)\n        \n        # Early stopping check\n        if avg_val_loss < best_val_loss:\n            best_val_loss = avg_val_loss\n            patience_counter = 0\n        else:\n            patience_counter += 1\n        \n        # Print progress\n        if epoch % 10 == 0 or epoch < 5:\n            avg_corr = np.mean(avg_correlations)\n            print(f\"Epoch {epoch:2d} | Train: {avg_train_loss:.4f} | Val: {avg_val_loss:.4f} | Corr: {avg_corr:.3f}\")\n        \n        # Early stopping\n        if patience_counter >= config.early_stopping_patience:\n            print(f\"Early stopping at epoch {epoch}\")\n            break\n    \n    # Final test evaluation\n    X_test_batches, y_test_batches = create_data_batches(\n        X_test, y_test, config.batch_size, jax.random.PRNGKey(0)\n    )\n    \n    test_losses = []\n    test_correlations = []\n    \n    for i in range(X_test_batches.shape[0]):\n        test_loss, correlations, _ = eval_step_fixed(\n            state, X_test_batches[i], y_test_batches[i]\n        )\n        test_losses.append(float(test_loss))\n        test_correlations.append(correlations)\n    \n    final_test_loss = np.mean(test_losses)\n    final_test_corr = np.mean(np.stack(test_correlations), axis=0)\n    \n    results = {\n        'model_name': model_name,\n        'train_losses': train_losses,\n        'val_losses': val_losses,\n        'val_correlations': val_correlations,\n        'test_loss': final_test_loss,\n        'test_correlations': final_test_corr,\n        'avg_test_correlation': float(np.mean(final_test_corr)),\n        'final_epoch': epoch,\n        'params': state.params\n    }\n    \n    print(f\"\\n‚úÖ {model_name} Final Results:\")\n    print(f\"   Test Loss: {final_test_loss:.4f}\")\n    print(f\"   Avg Correlation: {results['avg_test_correlation']:.3f}\")\n    print(f\"   Best Dimensions: {np.argsort(final_test_corr)[-3:]}\")\n    \n    return results\n\n# Test all architectures with fixed function\nmodel_configs = [\n    (\"Standard CNN\", PianoSpectroCNN),\n    (\"RealTime CNN\", RealTimePianoCNN)\n]\n\nresults = {}\n\nfor model_name, model_class in model_configs:\n    try:\n        result = train_and_evaluate_model_fixed(\n            model_name, model_class, config,\n            X_train, y_train, X_val, y_val, X_test, y_test\n        )\n        results[model_name] = result\n        \n    except Exception as e:\n        print(f\"‚ùå {model_name} failed: {e}\")\n        import traceback\n        traceback.print_exc()\n        results[model_name] = None\n\nprint(f\"\\nüéØ ARCHITECTURE COMPARISON COMPLETE!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "results_viz"
   },
   "source": [
    "# üìà Results Analysis & Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "analyze_results"
   },
   "outputs": [],
   "source": [
    "# Training curves comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Loss curves\n",
    "for model_name, result in results.items():\n",
    "    if result is not None:\n",
    "        axes[0, 0].plot(result['train_losses'], label=f'{model_name} Train', alpha=0.8)\n",
    "        axes[0, 1].plot(result['val_losses'], label=f'{model_name} Val', alpha=0.8)\n",
    "\n",
    "axes[0, 0].set_title('Training Losses')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('MSE Loss')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[0, 1].set_title('Validation Losses')\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('MSE Loss')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Final performance comparison\n",
    "model_names = [name for name, result in results.items() if result is not None]\n",
    "test_losses = [result['test_loss'] for name, result in results.items() if result is not None]\n",
    "avg_correlations = [result['avg_test_correlation'] for name, result in results.items() if result is not None]\n",
    "\n",
    "x_pos = np.arange(len(model_names))\n",
    "\n",
    "axes[1, 0].bar(x_pos, test_losses, alpha=0.7, color=['skyblue', 'orange'])\n",
    "axes[1, 0].set_title('Final Test Loss (Lower = Better)')\n",
    "axes[1, 0].set_xlabel('Model')\n",
    "axes[1, 0].set_ylabel('MSE Loss')\n",
    "axes[1, 0].set_xticks(x_pos)\n",
    "axes[1, 0].set_xticklabels(model_names, rotation=45)\n",
    "\n",
    "# Add value labels\n",
    "for i, v in enumerate(test_losses):\n",
    "    axes[1, 0].text(i, v + 0.001, f'{v:.4f}', ha='center', va='bottom')\n",
    "\n",
    "axes[1, 1].bar(x_pos, avg_correlations, alpha=0.7, color=['lightgreen', 'gold'])\n",
    "axes[1, 1].set_title('Average Test Correlation (Higher = Better)')\n",
    "axes[1, 1].set_xlabel('Model')\n",
    "axes[1, 1].set_ylabel('Correlation')\n",
    "axes[1, 1].set_xticks(x_pos)\n",
    "axes[1, 1].set_xticklabels(model_names, rotation=45)\n",
    "\n",
    "# Add value labels\n",
    "for i, v in enumerate(avg_correlations):\n",
    "    axes[1, 1].text(i, v + 0.01, f'{v:.3f}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Per-dimension correlation analysis\n",
    "if len(results) > 0:\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    \n",
    "    dimension_names = data_generator.dimensions\n",
    "    x_dims = np.arange(19)\n",
    "    \n",
    "    for i, (model_name, result) in enumerate(results.items()):\n",
    "        if result is not None:\n",
    "            plt.bar(x_dims + i*0.35, result['test_correlations'], \n",
    "                   width=0.35, alpha=0.7, label=model_name)\n",
    "    \n",
    "    plt.title('Per-Dimension Test Correlations')\n",
    "    plt.xlabel('Perceptual Dimension')\n",
    "    plt.ylabel('Correlation')\n",
    "    plt.xticks(x_dims + 0.175, [d[:15] + '...' for d in dimension_names], rotation=45, ha='right')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\nüìä VALIDATION RESULTS SUMMARY:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for model_name, result in results.items():\n",
    "    if result is not None:\n",
    "        print(f\"\\n{model_name}:\")\n",
    "        print(f\"  ‚úì Training completed successfully\")\n",
    "        print(f\"  ‚úì Final test loss: {result['test_loss']:.4f}\")\n",
    "        print(f\"  ‚úì Average correlation: {result['avg_test_correlation']:.3f}\")\n",
    "        print(f\"  ‚úì Converged in {result['final_epoch']} epochs\")\n",
    "    else:\n",
    "        print(f\"\\n{model_name}: ‚ùå FAILED\")\n",
    "\n",
    "# Success criteria check\n",
    "success_criteria = {\n",
    "    'models_trained': len([r for r in results.values() if r is not None]) >= 1,\n",
    "    'loss_decreased': all(r['val_losses'][-1] < r['val_losses'][0] for r in results.values() if r is not None),\n",
    "    'correlations_reasonable': all(r['avg_test_correlation'] > 0.5 for r in results.values() if r is not None)\n",
    "}\n",
    "\n",
    "print(f\"\\nüéØ SUCCESS CRITERIA:\")\n",
    "for criterion, passed in success_criteria.items():\n",
    "    status = \"‚úÖ PASS\" if passed else \"‚ùå FAIL\"\n",
    "    print(f\"   {criterion}: {status}\")\n",
    "\n",
    "all_passed = all(success_criteria.values())\n",
    "print(f\"\\nüöÄ Overall Status: {'READY FOR REAL DATA!' if all_passed else 'NEEDS DEBUGGING'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "next_steps"
   },
   "source": [
    "# üéµ Next Steps: Chopin/Liszt Data Collection\n",
    "\n",
    "## If validation successful:\n",
    "\n",
    "### 1. Audio Collection Strategy\n",
    "```python\n",
    "# Target repertoire for diversity\n",
    "chopin_pieces = [\n",
    "    \"Etude Op.10 No.1-12\",  # Technical variety\n",
    "    \"Nocturne Op.9 No.1-3\", # Expressive range\n",
    "    \"Ballade No.1-4\",       # Structural complexity\n",
    "    \"Prelude Op.28 (selection)\" # Stylistic diversity\n",
    "]\n",
    "\n",
    "liszt_pieces = [\n",
    "    \"Hungarian Rhapsody No.2, 6\", # Dramatic expression\n",
    "    \"Liebestraum No.3\",          # Lyrical style\n",
    "    \"Transcendental Etude 4, 10\" # Technical brilliance\n",
    "]\n",
    "```\n",
    "\n",
    "### 2. Recording Sources\n",
    "- **YouTube**: Concert recordings, competitions\n",
    "- **IMSLP**: Public domain recordings\n",
    "- **Personal**: Your own performances\n",
    "- **Target**: 10-15 different interpreters per piece\n",
    "\n",
    "### 3. Labeling Interface\n",
    "```python\n",
    "# Simple rating interface\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "def create_rating_interface(audio_file, dimensions):\n",
    "    # Audio playback + 19 sliders for ratings\n",
    "    pass\n",
    "```\n",
    "\n",
    "### 4. Quality Control\n",
    "- Rate same performance twice (consistency check)\n",
    "- Use subset of PercePiano for calibration\n",
    "- Multiple raters for subset validation\n",
    "\n",
    "## Expected Timeline\n",
    "- **Week 1**: Audio collection (20-30 pieces)\n",
    "- **Week 2**: Initial labeling + rating interface\n",
    "- **Week 3**: Full dataset creation (100+ performances)\n",
    "- **Week 4**: Model training on real Chopin/Liszt data\n",
    "\n",
    "**Ready to move forward with real data collection! üéπ**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "model_export"
   },
   "source": [
    "# üíæ Model Export for Real Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "export_models"
   },
   "outputs": [],
   "source": [
    "# Save validation results and best model\n",
    "validation_summary = {\n",
    "    'synthetic_data_stats': {\n",
    "        'num_samples': X_synthetic.shape[0],\n",
    "        'spectrogram_shape': X_synthetic.shape[1:],\n",
    "        'num_dimensions': y_synthetic.shape[1]\n",
    "    },\n",
    "    'model_results': {}\n",
    "}\n",
    "\n",
    "# Find best model\n",
    "best_model = None\n",
    "best_correlation = 0\n",
    "\n",
    "for model_name, result in results.items():\n",
    "    if result is not None:\n",
    "        validation_summary['model_results'][model_name] = {\n",
    "            'test_loss': float(result['test_loss']),\n",
    "            'avg_correlation': float(result['avg_test_correlation']),\n",
    "            'final_epoch': int(result['final_epoch'])\n",
    "        }\n",
    "        \n",
    "        if result['avg_test_correlation'] > best_correlation:\n",
    "            best_correlation = result['avg_test_correlation']\n",
    "            best_model = model_name\n",
    "\n",
    "validation_summary['best_model'] = best_model\n",
    "validation_summary['validation_passed'] = all(success_criteria.values())\n",
    "\n",
    "# Export results\n",
    "with open('piano_colab_validation_results.json', 'w') as f:\n",
    "    json.dump(validation_summary, f, indent=2)\n",
    "\n",
    "print(f\"\\nüíæ Results exported to: piano_colab_validation_results.json\")\n",
    "print(f\"\\nüèÜ Best performing model: {best_model} (correlation: {best_correlation:.3f})\")\n",
    "\n",
    "if validation_summary['validation_passed']:\n",
    "    print(f\"\\nüéâ VALIDATION SUCCESSFUL! Ready for Chopin/Liszt data collection!\")\n",
    "    print(f\"\\nüéØ Next steps:\")\n",
    "    print(f\"   1. Collect 20-30 diverse Chopin/Liszt recordings\")\n",
    "    print(f\"   2. Create rating interface for perceptual labeling\")\n",
    "    print(f\"   3. Train {best_model} on real data\")\n",
    "    print(f\"   4. Compare with PercePiano baseline\")\n",
    "else:\n",
    "    print(f\"\\nüîß Validation issues detected - debug before proceeding\")\n",
    "    \n",
    "# Download results file\n",
    "from google.colab import files\n",
    "files.download('piano_colab_validation_results.json')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}